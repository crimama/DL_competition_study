# 4회차 - 베이스라인 디벨롭 2 

# 미션 
- 3회차 까지 사실상 기본적인 베이스라인 구축을 했다고 생각할 수 있습니다. 
- 기본적인 베이스라인을 구축했다면 이제 모델을 디벨롭 하고, 하이퍼 파라미터 튜닝을 해볼 시간입니다. 
- 모델 디벨롭은 조금 더 많은 시간과 이론을 필요로 하니 하이퍼 파라미터 튜닝을 해보고자 합니다. 
- 하이퍼 파라미터 튜닝이란 전체적인 모델 구조는 유지한 채 세부 요소를 변화를 주어 최적의 성능을 도출하는 것을 의미합니다. 
- 하이퍼 파라미터 튜닝의 주 대상은 다음과 같습니다. 
  - Optimizer 
  - Learning rate 
  - Scheduler 
  - Pretrained model 
  - Loss function 
  - AMP(auto mixed precision)

# 1단계 - Optimizer 
- Optimizer란 간단하게 말해 모델 학습의 최적화를 도와주는 방식이라 생각하면 됩니다. 
- 딥러닝을 다른 방식으로 이해 하면 실제값과 예측값을 최소화 하는 파라미터를 찾는 과정이라 생각할 수 있는데 이 과정을 설계하는 것이 Optimizer입니다. 
- 조금 더 간단하게 이야기 해 보면 최적의 파라미터를 찾기 위해 현재의 파라미터를 얼마나 어디로 줄이고 늘려주냐 정도로 이해할 수 있을 것 같습니다. 
- 딥러닝을 하다 보면 `Adam` 이라는 단어를 많이 들어보실 수 있는데 `Adam`이 바로 Optimizer입니다. 
- `Adam`은 가장 많이 사용되는 Optimizer입니다. 그 만큼 성능이 매우 우수하며 어떠한 Task, Data 라도 준수한 성능을 보여줍니다. 
- 하지만 항상 `Adam`이 좋은 성능을 보여주는 것은 아닙니다. 그렇기에 다른 Optimizer 종류를 알고 비교 실험을  해서 최적의 Optimizer를 선택해야 합니다. 

**Optimizer 종류** 
- Adam 
- Adagrad
- RMSprop
- SGD
- AdamW
- RAdam
- 등등... 

**To do** 
- Adam이 아닌 다른 Optimizer 사용 
- 다른 Optimizer를 사용했을 때 성능 비교 

# 2단계 - Learning rate 
- 앞선 1단계에서는 Optimizer에 대해서 다뤄 보았습니다. Optimizer를 언급하게 되면 항상 뒤 따라오는 것이 있는데 바로 Learning rate입니다. 
- Optimizer가 최적의 파라미터를 찾기 위한 과정, 방법이라고 한다면 그 과정에서 한번 변화를 줄때 얼마나 적게 혹은 얼마나 많이 줄여 주냐를 결정 짓는 것이 Learning rate입니다. 
- Learning rate가 클 수록 한 번에 파라미터 변화를 크게 주는 것이며 반대로 Learning rate를 작게 줄 수록 조금씩 천천히 변화를 주게 됩니다. 
- Learning rate가 클 수록 빠르게 수렴을 할 수 있지만 되려 너무 큰 탓에 최적 지점을 벗어나 발산을 할 수도 있습니다. 
- 반대로 작을수록 모든 값들을 확인하며 최적의 값을 찾을 수 있지만 되려 너무 작은 탓에 Local minimum에 빠질 수도 있습니다. 
- 이 역시 Optimizer와 함께 값을 변화 해 주며 최적의 값을 찾는 것이 중요합니다. 

**To do**
- 0.1 부터 0.00001 까지 변화를 주어 성능 비교 
  

# 3단계 - Scheduler 
- Scheduler는 말 그대로 계획을 짜 주는 역할을 합니다. 위에서 Optimizer랑 Learning rate를 설정 해 두었지만 파라미터의 상황에 따라 많이 변화를 해야할 때가 있고 반대로 적게 변화를 해야 할 때가 있습니다. 
- 하지만 딥러닝을 학습하는 과정에서는 중간에 간섭을 할 수가 없죠. 
- 그래서 해당 모델 학습 상황에 맞춰 Learning rate를 조절해주는 요소가 Scheduler입니다. 
- 종류는 코사인 어닐링, 코사인 웜업, Exponential 등 다양하게 있습니다. 

